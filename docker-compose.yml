services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    volumes:
      - ./backend:/app
      - ./database:/app/database # Mount the ChromaDB database
      - ./logging_database.db:/app/logging_database.db # Mount the logging database
    env_file:
      - ./.env
    healthcheck:
      test: [ "CMD", "python", "healthcheck.py" ]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    networks:
      - app-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "8501:8501"
    volumes:
      - ./frontend:/app
    environment:
      - API_URL=http://backend:8000
    depends_on:
      backend:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - app-network

  # Separate service for pipeline tasks
  # This will not start automatically, but can be run with:
  # docker-compose run pipeline
  pipeline:
    build:
      context: ./backend
      dockerfile: Dockerfile.pipeline
    volumes:
      - ./backend:/app
      - ./database:/app/database # Mount the ChromaDB database
      - ./logging_database.db:/app/logging_database.db # Logging database
      - ./URLs.txt:/app/URLs.txt # Tracking file for processed URLs
      - data_scraping_downloads:/app/data_scraping/downloads # Persistent download location
    env_file:
      - ./.env
    restart: "no" # Don't restart automatically
    # Add health check after pipeline completes to ensure it didn't corrupt the database
    healthcheck:
      test: [ "CMD", "python", "healthcheck.py" ]
      interval: 60s
      timeout: 20s
      retries: 1
      start_period: 30s
    networks:
      - app-network

networks:
  app-network:
    driver: bridge

volumes:
  data_scraping_downloads:
    driver: local
